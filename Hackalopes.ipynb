{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Energy A.I. Hackathon 2023 Workflow - Hackalopes \n",
    "\n",
    "#### Authors:   \n",
    "<h4>  \n",
    "\n",
    "**Richard Larson** - Energy and Earth Resources - *Jackson*  \n",
    "**Karthik Menon** - Energy and Earth Resources - *Jackson*   \n",
    "**Daniel Pang** - Petroleum and Geosystems Engineering - *Cockrell*    \n",
    "**Benjamin Stormer** - Walker Department of Mechanical Engineering - *Cockrell* \n",
    "\n",
    "</h4> \n",
    "\n",
    "#### The University of Texas at Austin, Austin, Texas USA \n",
    "___\n",
    "\n",
    "### Executive Summary \n",
    "\n",
    "We needed to predict whether or not 40 Electronic Submersible Pumps (ESP) will fail or not within 30 days. To create our predictions, we developed a data analytics and machine learning workflow in Python. We found that the GOR, ESP vibration data, and other factors related to fluid flow through the pump are most critical in assessing the status of an ESP. We recommend that feature engineering of these factors and ... be used as a measure to evaluate the lifespan of ESPs in the future.\n",
    "\n",
    "___\n",
    "\n",
    "### Workflow Goal\n",
    "\n",
    "Our workflow should clearly provide the steps to select the optimal features for evaluating ESP lifespan, impute or remove any appropriate data, and develop a predictive learning model to categorize ESPs as expected to fail or not to fail in a period of 30 days.\n",
    "___\n",
    "\n",
    "### Workflow Steps \n",
    "\n",
    "Enumerated steps, very short and concise overview of your methods and workflow\n",
    "\n",
    "1. **Data Analysis** - Separate data in failing pumps from operational pumps and understand feature completeness and correlation\n",
    "2. **Feature Selection** - Leverage physical knowledge and domain expertise to determine most influential features, while imputing any data when possible, and removing any redundancies when necessary.\n",
    "3. **Machine Learning Model \\#1** - Create a minimum viable prototype using simple machine learning methods\n",
    "4. **Machine Learning Model \\#2** - Improve the minimum viable prototype by increasing complexity and changing feature selections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                    # model arrays\n",
    "import pandas as pd                   # DataFrames\n",
    "import matplotlib.pyplot as plt       # building plots\n",
    "import seaborn as sns                 # Plotting help\n",
    "import os                             # accessing the operating system\n",
    "from sklearn.impute import KNNImputer # k-nearest imputing\n",
    "from sklearn.experimental import enable_iterative_imputer # required for MICE imputation\n",
    "from sklearn.impute import IterativeImputer # MICE imputation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The following workflow applies the .csv files 'wellData.csv' and 'dailyData.csv'. 'wellData.csv' is a collection of static features related to a specific well. 'dailyData.csv' is a collection of time-series data particular to the operation of each pump. The datasets were made available to competitors as part of the UT Austin 2023 PGE Hackathon.\n",
    "\n",
    "We will work with the following features:\n",
    "* **DLS_Critical** - ...\n",
    "* **GOR** - ...\n",
    "* **ESP Data - Vibration X** - ...\n",
    "* **Pump_Power** - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative paths used since assumption is that user is in root directory of forked Git repo.\n",
    "well_data = pd.read_csv(\"../Hackalopes/wellData.csv\")      # load the well data in\n",
    "daily_data = pd.read_csv(\"../Hackalopes/dailyData.csv\")    # load the daily data in\n",
    "solution_data = pd.read_csv(\"../Hackalopes/solution.csv\")  # load the solution data in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "The following functions will be used in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots a correlation matrix as a heat map\n",
    "def plot_corr(dataframe,size=10):                        \n",
    "    corr = dataframe.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    im = ax.matshow(corr,vmin = -1.0, vmax = 1.0)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.colorbar(im, orientation = 'vertical')\n",
    "    plt.title('Correlation Matrix')\n",
    "\n",
    "\n",
    "# Replaces the NaN values in a dataset\n",
    "def count_and_replace_nan(dataset, feature, replacement_value):\n",
    "    num_nan = sum(dataset[feature].isna())\n",
    "    print(f\"Number of NaN values in {feature} is {num_nan} out of {len(dataset)} values\")\n",
    "    if replacement_value == \"bf\":\n",
    "        new = dataset[feature].fillna(method=\"bfill\", inplace=True)\n",
    "        print(f\"NaN values have been replaced by back-filling\")\n",
    "    elif replacement_value == \"ff\":\n",
    "        dataset[feature].fillna(method=\"ffill\", inplace=True)\n",
    "        print(f\"NaN values have been replaced by forward-filling\")\n",
    "    else:\n",
    "        dataset[feature].fillna(replacement_value, inplace=True)\n",
    "        print(f\"NaN values have been replaced with {replacement_value}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Compute the completeness metrics for a given dataset\n",
    "def completeness(df, cutoff=0.75):\n",
    "    df_temp = df.copy(deep=True)\n",
    "    df_bool = df_temp.isnull()\n",
    "    features = list(df_bool.columns)\n",
    "    percent_missing = []\n",
    "    past_cutoff = []\n",
    "    for feature in features:\n",
    "        num_missing = df_bool[feature].sum()\n",
    "        proportion_missing = df_bool[feature].sum() / len(df_bool)\n",
    "        percent_missing.append(proportion_missing)\n",
    "        if proportion_missing >= cutoff:\n",
    "            past_cutoff.append(feature)\n",
    "    print(f'{past_cutoff} were missing at least {cutoff*100}% of their data')\n",
    "\n",
    "\n",
    "# Calculate the (spearman) correlation between all features in a dataframe.\n",
    "def correlation(df, cutoff=0.75):\n",
    "    corr = df.corr(method='spearman')\n",
    "    correlated = []\n",
    "    inversely_correlated = []\n",
    "    for feature in corr:\n",
    "        for feature2 in corr:\n",
    "            correlation = corr[feature][feature2]\n",
    "            if str(feature) == str(feature2):\n",
    "                pass\n",
    "            elif correlation >= cutoff:\n",
    "                correlated.append((feature, feature2))\n",
    "            elif correlation <= -cutoff:\n",
    "                inversely_correlated.append((feature, feature2))\n",
    "    # print(f'Items listed here: {correlated} are correlated by a coefficient of at least {cutoff}')\n",
    "    # print('-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    # print(f'Items listed here: {inversely_correlated} are inversely correlated by a coefficient of at greatest -{cutoff}')\n",
    "\n",
    "\n",
    "    # Function to separate a feature by time intervals\n",
    "def separate_intervals(dataset, feature):\n",
    "    grouped_data=dataset.groupby([\"Well_ID\",\"AL_Key\"])\n",
    "    array_life=np.array(grouped_data.size())\n",
    "    mega_matrix=np.array(grouped_data)\n",
    "    test_array=np.array(dataset) # Get the Well_ID\n",
    "    list_sort=[]\n",
    "    list_fail=[]\n",
    "    for i in range (0,len(mega_matrix)):\n",
    "        flag=0\n",
    "        for k in range(0,len(test_array)):        \n",
    "            if mega_matrix[i][0][0] == test_array[k][0] and mega_matrix[i][0][1] == test_array[k][1]:\n",
    "                list_sort.append(i)\n",
    "                flag=1            \n",
    "        if flag==0:\n",
    "            list_fail.append(i)\n",
    "    mega_s=mega_matrix[list_sort]\n",
    "    mega_s_life=array_life[list_sort]\n",
    "    \n",
    "    mega_f=mega_matrix[list_fail]\n",
    "    mega_f_life=array_life[list_fail]\n",
    "\n",
    "    old_folks=[]\n",
    "    for i in range (len(mega_f)):\n",
    "        overall_time=mega_f_life[i]\n",
    "        senior=overall_time-int((overall_time*0.1).round(0))\n",
    "        alpha=mega_f[i][1][senior:overall_time][feature].diff().mean()\n",
    "        old_folks.append(alpha)\n",
    "\n",
    "    army=[]\n",
    "    for i in range (len(mega_f)):\n",
    "        overall_time=mega_f_life[i]\n",
    "        start=overall_time-int((overall_time*0.6).round(0))\n",
    "        end=overall_time-int((overall_time*0.4).round(0))\n",
    "        soldier=mega_f[i][1][start:end][feature].diff().mean()\n",
    "        army.append(soldier)\n",
    "        \n",
    "    return([old_folks, army])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate and Separate Data Appropriately\n",
    "\n",
    "The following steps combine the input csv files into DataFrames that are more useful to manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark data we will be predicting with a 0 and failed pumps with a 1\n",
    "solution_data = solution_data.rename(columns={\"Fail in 30 days\": \"Failed\"}) \n",
    "solution_data[\"Failed\"] = 0 # Initialize solutions with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data into one dataframe\n",
    "combined_data = pd.merge(pd.merge(well_data, daily_data, on=[\"Well_ID\", \"AL_Key\"], how=\"left\"), \n",
    "                         solution_data, on=[\"Well_ID\", \"AL_Key\"], how=\"left\")\n",
    "combined_data[\"Failed\"] = combined_data[\"Failed\"].replace(np.nan, 1)\n",
    "combined_data = combined_data.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backfill NaN values in OIL, GAS, and Water columns\n",
    "combined_data[\"OIL\"].fillna(method=\"bfill\", inplace=True)\n",
    "combined_data[\"GAS\"].fillna(method=\"bfill\", inplace=True)\n",
    "combined_data[\"WATER\"].fillna(method=\"bfill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new ratios and features in dataframe\n",
    "combined_data.insert(15, \"GOR\", combined_data[\"GAS\"] / combined_data[\"OIL\"])\n",
    "combined_data.insert(16, \"GOF\", combined_data[\"GAS\"] / (combined_data[\"OIL\"] + combined_data[\"WATER\"]))\n",
    "combined_data.insert(17, \"GOR_Slope\", combined_data[\"GOR\"].diff())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for both the pumps still operational and the pumps that have failed\n",
    "solution_data = combined_data[combined_data[\"Failed\"] == 0] # Still operational\n",
    "failed_data = combined_data[combined_data[\"Failed\"] == 1] # Failed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Checking and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(combined_data) # Compute the correlation statistics for the combined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ESP_Motor_Frequency_Rating'] were missing at least 75.0% of their data\n"
     ]
    }
   ],
   "source": [
    "completeness(solution_data) # Compute the data completeness statistics for the operational pumps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Statistics for the Minimum Viable Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove inessential data for a preliminary prototype. Essential data \n",
    "# determined from physical knowldege from domain experts and \n",
    "# correlation/completeness studies.\n",
    "min_viable = combined_data.drop(columns=['Artificial_Lift_Type',\n",
    "       'AL_Bottom_Depth', 'ESP_Pump_Stages',\n",
    "       'ESP_Motor_Frequency_Rating', 'ESP_Motor_Current_Rating',\n",
    "       'ESP_Motor_Voltage_Rating', 'ESP_Motor_Power_Rating',\n",
    "       'DLS_at_Set_Depth', 'OIL', 'GAS', 'WATER', 'ARTIFICIAL_LIFT',\n",
    "       'DOWN_TIME_HOURS', 'ESP Data - Drive Current',\n",
    "       'ESP Data - Drive Voltage', 'ESP Data - Intake Pressure',\n",
    "       'ESP Data - Motor Temperature Shutdown Setpoint',\n",
    "       'ESP Data - Motor Winding Temperature', 'ESP Data - Output Frequency',\n",
    "       'Startup_Count', 'Oil_Intake', 'Water_Intake',\n",
    "       'Gas_Intake', 'Liquid_Intake', 'Gas_Saturation_at_Intake',\n",
    "       'Gas_Separator_Efficiency', 'Gas_through_Annulus_Intake',\n",
    "       'Gas_through_ESP_Intake', 'Gas_through_Annulus', 'Gas_through_ESP',\n",
    "       'Pb_ESP', 'Discharge_Pressure', 'ESP_Fluid',\n",
    "       'Gas_Saturation_at_Discharge', 'Pump_Delta_Pressure',\n",
    "       'Pump_Average_Pressure', 'Gas_Saturation_in_Pump',\n",
    "       'Drive_Power', 'Power_Ratio', 'Power_Difference', 'ESP_Temperature',\n",
    "       'Lower_Limit', 'Failed'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Statistics for the Improved Model\n",
    "Two methods of imputation are used and tested separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove inessential data for an improved prototype. This is slightly less\n",
    "# strict in removing features then the minimum viable prototype\n",
    "improved = combined_data.drop(columns=['Artificial_Lift_Type',\n",
    "       'AL_Bottom_Depth', 'ESP_Pump_Stages',\n",
    "       'ESP_Motor_Frequency_Rating', 'ESP_Motor_Current_Rating',\n",
    "       'ESP_Motor_Voltage_Rating', 'ESP_Motor_Power_Rating',\n",
    "       'DLS_at_Set_Depth', 'GAS', 'WATER', 'ARTIFICIAL_LIFT',\n",
    "       'DOWN_TIME_HOURS', 'ESP Data - Drive Current',\n",
    "       'ESP Data - Drive Voltage', 'ESP Data - Intake Pressure',\n",
    "       'ESP Data - Motor Temperature Shutdown Setpoint',\n",
    "       'ESP Data - Motor Winding Temperature', 'ESP Data - Output Frequency',\n",
    "       'Startup_Count', 'Oil_Intake', 'Water_Intake',\n",
    "       'Gas_Intake', 'Liquid_Intake', 'Gas_Saturation_at_Intake',\n",
    "       'Gas_Separator_Efficiency', 'Gas_through_Annulus_Intake',\n",
    "       'Gas_through_ESP_Intake', 'Gas_through_Annulus', 'Gas_through_ESP',\n",
    "       'Discharge_Pressure', 'ESP_Fluid',\n",
    "       'Gas_Saturation_at_Discharge', 'Pump_Delta_Pressure',\n",
    "       'Pump_Average_Pressure', 'Gas_Saturation_in_Pump',\n",
    "       'Drive_Power', 'Power_Difference', 'ESP_Temperature',\n",
    "       'Lower_Limit', 'Failed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code modified from https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Feature_Imputation.ipynb\n",
    "# Authored by Dr. Michael Pyrcz\n",
    "\n",
    "# Impute the data using k-nearest neighbors\n",
    "df_knn = improved.copy(deep=True) # make a deep copy of the DataFrame\n",
    "df_knn.drop(columns=[\"Well_ID\", \"AL_Key\"], inplace=True)\n",
    "df_knn.replace([np.inf, -np.inf], np.nan, inplace=True) # Replace infinite values\n",
    "knn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "df_knn.iloc[:,:] = knn_imputer.fit_transform(df_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code modified from https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Feature_Imputation.ipynb\n",
    "# Authored by Dr. Michael Pyrcz\n",
    "\n",
    "# Impute the data using chained equations\n",
    "df_mice = improved.copy(deep=True)  # make a deep copy of the DataFrame\n",
    "df_mice.drop(columns=[\"Well_ID\", \"AL_Key\"], inplace=True)\n",
    "df_mice.replace([np.inf, -np.inf], np.nan, inplace=True) # Replace infinite values\n",
    "mice_imputer = IterativeImputer()\n",
    "df_mice.iloc[:,:] = mice_imputer.fit_transform(df_mice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "043433d6b18bdeb8544c06540320f4c494e406f840db1fa6d3d1188f7786a55f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
